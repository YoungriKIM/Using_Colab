{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras18_boston3_minmaxScarlar_re.py",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHFchWCnv2q+YNN/90144D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungriKIM/Using_Colab/blob/main/keras18_boston3_minmaxScarlar_re_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iswsWiOQts8K",
        "outputId": "bb700443-1e75-4b8b-df61-6669e22f8b4e"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "#데이터 먼저 불러와야 겠지\r\n",
        "from sklearn.datasets import load_boston\r\n",
        "dataset = load_boston()\r\n",
        "x = dataset.data\r\n",
        "y = dataset.target\r\n",
        "\r\n",
        "print(x.shape) #(506, 13)\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "scaler = MinMaxScaler()\r\n",
        "scaler.fit(x)\r\n",
        "x = scaler.transform(x)\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=311)\r\n",
        "\r\n",
        "#모델 구성\r\n",
        "from tensorflow.keras.models import Sequential, Model\r\n",
        "from tensorflow.keras.layers import Dense, Input\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Dense(40,input_shape=(13,) , activation='relu'))\r\n",
        "model.add(Dense(30))\r\n",
        "model.add(Dense(30))\r\n",
        "model.add(Dense(20))\r\n",
        "model.add(Dense(20))\r\n",
        "model.add(Dense(1))\r\n",
        "\r\n",
        "#컴파일, 훈련\r\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\r\n",
        "model.fit(x_train, y_train, epochs=100, batch_size=2, validation_split=0.2, verbose=2)\r\n",
        "\r\n",
        "#평가, 예측\r\n",
        "loss, mae = model.evaluate(x_test, y_test, batch_size=1)\r\n",
        "print('loss, mae: ', loss, mae)\r\n",
        "\r\n",
        "y_predict = model.predict(x_test)\r\n",
        "\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "def RMSE(y_test, y_predict):\r\n",
        "    return np.sqrt(mean_squared_error(y_test, y_predict))\r\n",
        "print('RMSE: ', RMSE(y_test, y_predict))\r\n",
        "\r\n",
        "from sklearn.metrics import r2_score\r\n",
        "R2 = r2_score(y_test, y_predict)\r\n",
        "print('R2: ', R2)\r\n",
        "\r\n",
        "# 18-3 기준:전체x\r\n",
        "# loss, mae:  14.40992259979248 2.5567142963409424\r\n",
        "# RMSE:  3.79604081933286\r\n",
        "# R2:  0.8004349233068114"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(506, 13)\n",
            "Epoch 1/100\n",
            "162/162 - 1s - loss: 167.1622 - mae: 9.5291 - val_loss: 84.0262 - val_mae: 6.0634\n",
            "Epoch 2/100\n",
            "162/162 - 0s - loss: 42.5752 - mae: 4.7222 - val_loss: 41.2438 - val_mae: 5.0237\n",
            "Epoch 3/100\n",
            "162/162 - 0s - loss: 28.9136 - mae: 3.8729 - val_loss: 33.9545 - val_mae: 4.2843\n",
            "Epoch 4/100\n",
            "162/162 - 0s - loss: 24.3694 - mae: 3.4171 - val_loss: 46.9575 - val_mae: 4.5402\n",
            "Epoch 5/100\n",
            "162/162 - 0s - loss: 22.3568 - mae: 3.3149 - val_loss: 27.5656 - val_mae: 3.5849\n",
            "Epoch 6/100\n",
            "162/162 - 0s - loss: 20.9844 - mae: 3.2554 - val_loss: 31.1610 - val_mae: 3.5186\n",
            "Epoch 7/100\n",
            "162/162 - 0s - loss: 22.9193 - mae: 3.3695 - val_loss: 23.9032 - val_mae: 3.2455\n",
            "Epoch 8/100\n",
            "162/162 - 0s - loss: 19.3614 - mae: 3.1470 - val_loss: 23.3185 - val_mae: 3.1659\n",
            "Epoch 9/100\n",
            "162/162 - 0s - loss: 17.8659 - mae: 2.9689 - val_loss: 22.5664 - val_mae: 2.8826\n",
            "Epoch 10/100\n",
            "162/162 - 0s - loss: 16.1593 - mae: 2.8752 - val_loss: 20.2132 - val_mae: 2.8098\n",
            "Epoch 11/100\n",
            "162/162 - 0s - loss: 16.3601 - mae: 2.8323 - val_loss: 20.0918 - val_mae: 2.8374\n",
            "Epoch 12/100\n",
            "162/162 - 0s - loss: 15.1963 - mae: 2.6773 - val_loss: 26.6078 - val_mae: 3.2833\n",
            "Epoch 13/100\n",
            "162/162 - 0s - loss: 14.8162 - mae: 2.7842 - val_loss: 22.3200 - val_mae: 2.9111\n",
            "Epoch 14/100\n",
            "162/162 - 0s - loss: 13.8040 - mae: 2.7556 - val_loss: 17.5614 - val_mae: 2.6890\n",
            "Epoch 15/100\n",
            "162/162 - 0s - loss: 13.9046 - mae: 2.6140 - val_loss: 17.1767 - val_mae: 2.5152\n",
            "Epoch 16/100\n",
            "162/162 - 0s - loss: 12.6543 - mae: 2.5479 - val_loss: 22.7519 - val_mae: 3.5336\n",
            "Epoch 17/100\n",
            "162/162 - 0s - loss: 13.3722 - mae: 2.6014 - val_loss: 20.2069 - val_mae: 3.1495\n",
            "Epoch 18/100\n",
            "162/162 - 0s - loss: 15.0016 - mae: 2.7932 - val_loss: 17.5505 - val_mae: 2.5821\n",
            "Epoch 19/100\n",
            "162/162 - 0s - loss: 12.1102 - mae: 2.5545 - val_loss: 16.7778 - val_mae: 2.5199\n",
            "Epoch 20/100\n",
            "162/162 - 0s - loss: 11.5207 - mae: 2.4365 - val_loss: 16.1374 - val_mae: 2.7787\n",
            "Epoch 21/100\n",
            "162/162 - 0s - loss: 11.0798 - mae: 2.4346 - val_loss: 16.6583 - val_mae: 2.5388\n",
            "Epoch 22/100\n",
            "162/162 - 0s - loss: 12.1000 - mae: 2.5536 - val_loss: 18.7381 - val_mae: 2.5096\n",
            "Epoch 23/100\n",
            "162/162 - 0s - loss: 10.1765 - mae: 2.3480 - val_loss: 18.5528 - val_mae: 2.5920\n",
            "Epoch 24/100\n",
            "162/162 - 0s - loss: 11.7456 - mae: 2.6194 - val_loss: 16.8314 - val_mae: 2.8532\n",
            "Epoch 25/100\n",
            "162/162 - 0s - loss: 8.8879 - mae: 2.1792 - val_loss: 17.6553 - val_mae: 3.0177\n",
            "Epoch 26/100\n",
            "162/162 - 0s - loss: 9.5862 - mae: 2.3294 - val_loss: 16.0812 - val_mae: 2.6825\n",
            "Epoch 27/100\n",
            "162/162 - 0s - loss: 10.7065 - mae: 2.4439 - val_loss: 16.7828 - val_mae: 2.5752\n",
            "Epoch 28/100\n",
            "162/162 - 0s - loss: 9.5584 - mae: 2.3191 - val_loss: 16.8109 - val_mae: 2.8793\n",
            "Epoch 29/100\n",
            "162/162 - 0s - loss: 10.2026 - mae: 2.4267 - val_loss: 16.3479 - val_mae: 2.5255\n",
            "Epoch 30/100\n",
            "162/162 - 0s - loss: 8.3203 - mae: 2.1560 - val_loss: 25.6176 - val_mae: 3.3168\n",
            "Epoch 31/100\n",
            "162/162 - 0s - loss: 10.0029 - mae: 2.3952 - val_loss: 16.0486 - val_mae: 2.5598\n",
            "Epoch 32/100\n",
            "162/162 - 0s - loss: 11.5207 - mae: 2.5755 - val_loss: 16.7218 - val_mae: 2.6329\n",
            "Epoch 33/100\n",
            "162/162 - 0s - loss: 8.1403 - mae: 2.1259 - val_loss: 18.5593 - val_mae: 2.6955\n",
            "Epoch 34/100\n",
            "162/162 - 0s - loss: 10.1287 - mae: 2.3634 - val_loss: 17.4981 - val_mae: 2.6267\n",
            "Epoch 35/100\n",
            "162/162 - 0s - loss: 8.8686 - mae: 2.2237 - val_loss: 17.9006 - val_mae: 2.7598\n",
            "Epoch 36/100\n",
            "162/162 - 0s - loss: 9.7331 - mae: 2.3590 - val_loss: 18.2472 - val_mae: 2.8752\n",
            "Epoch 37/100\n",
            "162/162 - 0s - loss: 7.6053 - mae: 2.0345 - val_loss: 17.4839 - val_mae: 2.5814\n",
            "Epoch 38/100\n",
            "162/162 - 0s - loss: 7.2461 - mae: 2.0641 - val_loss: 17.3766 - val_mae: 2.6034\n",
            "Epoch 39/100\n",
            "162/162 - 0s - loss: 7.0310 - mae: 2.0500 - val_loss: 20.2200 - val_mae: 3.1428\n",
            "Epoch 40/100\n",
            "162/162 - 0s - loss: 7.8340 - mae: 2.1701 - val_loss: 21.1125 - val_mae: 2.8483\n",
            "Epoch 41/100\n",
            "162/162 - 0s - loss: 8.9974 - mae: 2.2869 - val_loss: 16.8052 - val_mae: 2.5901\n",
            "Epoch 42/100\n",
            "162/162 - 0s - loss: 8.9800 - mae: 2.2774 - val_loss: 19.1938 - val_mae: 3.0174\n",
            "Epoch 43/100\n",
            "162/162 - 0s - loss: 8.3265 - mae: 2.2171 - val_loss: 19.0405 - val_mae: 2.7249\n",
            "Epoch 44/100\n",
            "162/162 - 0s - loss: 6.7666 - mae: 1.9646 - val_loss: 18.1455 - val_mae: 2.5790\n",
            "Epoch 45/100\n",
            "162/162 - 0s - loss: 7.7928 - mae: 2.1333 - val_loss: 21.6562 - val_mae: 3.2868\n",
            "Epoch 46/100\n",
            "162/162 - 0s - loss: 7.3271 - mae: 2.0511 - val_loss: 18.0638 - val_mae: 2.6762\n",
            "Epoch 47/100\n",
            "162/162 - 0s - loss: 7.0120 - mae: 1.9796 - val_loss: 19.7422 - val_mae: 2.7970\n",
            "Epoch 48/100\n",
            "162/162 - 0s - loss: 7.3646 - mae: 2.1355 - val_loss: 22.7861 - val_mae: 3.3172\n",
            "Epoch 49/100\n",
            "162/162 - 0s - loss: 6.8339 - mae: 2.0072 - val_loss: 20.7462 - val_mae: 2.8549\n",
            "Epoch 50/100\n",
            "162/162 - 0s - loss: 7.0362 - mae: 2.0264 - val_loss: 23.9614 - val_mae: 3.2959\n",
            "Epoch 51/100\n",
            "162/162 - 0s - loss: 6.4174 - mae: 1.9566 - val_loss: 19.8640 - val_mae: 2.7333\n",
            "Epoch 52/100\n",
            "162/162 - 0s - loss: 6.7154 - mae: 1.9048 - val_loss: 20.5238 - val_mae: 2.6742\n",
            "Epoch 53/100\n",
            "162/162 - 0s - loss: 6.3935 - mae: 1.9569 - val_loss: 21.1096 - val_mae: 2.8325\n",
            "Epoch 54/100\n",
            "162/162 - 0s - loss: 5.8084 - mae: 1.8702 - val_loss: 22.5355 - val_mae: 2.9812\n",
            "Epoch 55/100\n",
            "162/162 - 0s - loss: 6.2519 - mae: 1.9375 - val_loss: 22.6277 - val_mae: 3.1361\n",
            "Epoch 56/100\n",
            "162/162 - 0s - loss: 7.2167 - mae: 2.0682 - val_loss: 22.8945 - val_mae: 3.1435\n",
            "Epoch 57/100\n",
            "162/162 - 0s - loss: 7.4680 - mae: 2.0565 - val_loss: 20.7856 - val_mae: 2.9280\n",
            "Epoch 58/100\n",
            "162/162 - 0s - loss: 6.2433 - mae: 1.8998 - val_loss: 24.4236 - val_mae: 3.0666\n",
            "Epoch 59/100\n",
            "162/162 - 0s - loss: 6.1878 - mae: 1.8632 - val_loss: 21.5563 - val_mae: 2.9569\n",
            "Epoch 60/100\n",
            "162/162 - 0s - loss: 6.2942 - mae: 1.9241 - val_loss: 21.5716 - val_mae: 2.8881\n",
            "Epoch 61/100\n",
            "162/162 - 0s - loss: 6.2813 - mae: 1.9247 - val_loss: 20.9572 - val_mae: 2.8396\n",
            "Epoch 62/100\n",
            "162/162 - 0s - loss: 6.2401 - mae: 1.9620 - val_loss: 26.3051 - val_mae: 3.2645\n",
            "Epoch 63/100\n",
            "162/162 - 0s - loss: 6.8575 - mae: 1.9998 - val_loss: 21.0875 - val_mae: 3.0075\n",
            "Epoch 64/100\n",
            "162/162 - 0s - loss: 5.7752 - mae: 1.8573 - val_loss: 21.2058 - val_mae: 2.9069\n",
            "Epoch 65/100\n",
            "162/162 - 0s - loss: 6.5242 - mae: 1.9425 - val_loss: 23.2250 - val_mae: 2.9372\n",
            "Epoch 66/100\n",
            "162/162 - 0s - loss: 5.8979 - mae: 1.8353 - val_loss: 24.4014 - val_mae: 3.1375\n",
            "Epoch 67/100\n",
            "162/162 - 0s - loss: 6.4157 - mae: 1.8509 - val_loss: 23.6953 - val_mae: 2.9519\n",
            "Epoch 68/100\n",
            "162/162 - 0s - loss: 6.2960 - mae: 1.9509 - val_loss: 28.8906 - val_mae: 3.4921\n",
            "Epoch 69/100\n",
            "162/162 - 0s - loss: 5.8276 - mae: 1.8687 - val_loss: 22.3800 - val_mae: 2.9791\n",
            "Epoch 70/100\n",
            "162/162 - 0s - loss: 6.2400 - mae: 1.8762 - val_loss: 23.0924 - val_mae: 2.8832\n",
            "Epoch 71/100\n",
            "162/162 - 0s - loss: 7.1219 - mae: 2.0251 - val_loss: 24.6757 - val_mae: 3.0437\n",
            "Epoch 72/100\n",
            "162/162 - 0s - loss: 6.7723 - mae: 1.9788 - val_loss: 21.9060 - val_mae: 3.0324\n",
            "Epoch 73/100\n",
            "162/162 - 0s - loss: 6.0563 - mae: 1.8885 - val_loss: 22.8023 - val_mae: 2.8626\n",
            "Epoch 74/100\n",
            "162/162 - 0s - loss: 6.3774 - mae: 1.8741 - val_loss: 21.3492 - val_mae: 2.8236\n",
            "Epoch 75/100\n",
            "162/162 - 0s - loss: 6.0871 - mae: 1.9358 - val_loss: 22.4040 - val_mae: 2.8588\n",
            "Epoch 76/100\n",
            "162/162 - 0s - loss: 6.3701 - mae: 1.9817 - val_loss: 24.1062 - val_mae: 3.1418\n",
            "Epoch 77/100\n",
            "162/162 - 0s - loss: 6.2896 - mae: 1.9262 - val_loss: 27.2032 - val_mae: 3.3946\n",
            "Epoch 78/100\n",
            "162/162 - 0s - loss: 6.9801 - mae: 2.0300 - val_loss: 28.6674 - val_mae: 3.5439\n",
            "Epoch 79/100\n",
            "162/162 - 0s - loss: 7.5197 - mae: 2.1285 - val_loss: 24.1716 - val_mae: 3.1341\n",
            "Epoch 80/100\n",
            "162/162 - 0s - loss: 6.6248 - mae: 1.9125 - val_loss: 27.2859 - val_mae: 3.2766\n",
            "Epoch 81/100\n",
            "162/162 - 0s - loss: 6.0747 - mae: 1.8645 - val_loss: 23.5158 - val_mae: 2.9118\n",
            "Epoch 82/100\n",
            "162/162 - 0s - loss: 5.5417 - mae: 1.7892 - val_loss: 21.3408 - val_mae: 2.8672\n",
            "Epoch 83/100\n",
            "162/162 - 0s - loss: 5.7004 - mae: 1.8142 - val_loss: 23.1222 - val_mae: 2.9380\n",
            "Epoch 84/100\n",
            "162/162 - 0s - loss: 6.3298 - mae: 1.9510 - val_loss: 26.0737 - val_mae: 3.2975\n",
            "Epoch 85/100\n",
            "162/162 - 0s - loss: 5.9470 - mae: 1.8415 - val_loss: 22.9371 - val_mae: 3.0993\n",
            "Epoch 86/100\n",
            "162/162 - 0s - loss: 6.0465 - mae: 1.8473 - val_loss: 22.7928 - val_mae: 3.1286\n",
            "Epoch 87/100\n",
            "162/162 - 0s - loss: 5.7916 - mae: 1.8072 - val_loss: 21.5780 - val_mae: 2.9919\n",
            "Epoch 88/100\n",
            "162/162 - 0s - loss: 5.9591 - mae: 1.8589 - val_loss: 21.8667 - val_mae: 2.9387\n",
            "Epoch 89/100\n",
            "162/162 - 0s - loss: 5.4345 - mae: 1.8296 - val_loss: 23.1168 - val_mae: 3.0905\n",
            "Epoch 90/100\n",
            "162/162 - 0s - loss: 5.8840 - mae: 1.8282 - val_loss: 24.0995 - val_mae: 3.0744\n",
            "Epoch 91/100\n",
            "162/162 - 0s - loss: 5.4305 - mae: 1.7593 - val_loss: 23.5665 - val_mae: 3.1179\n",
            "Epoch 92/100\n",
            "162/162 - 0s - loss: 5.3588 - mae: 1.7472 - val_loss: 24.0810 - val_mae: 2.9923\n",
            "Epoch 93/100\n",
            "162/162 - 0s - loss: 5.5279 - mae: 1.8100 - val_loss: 23.6334 - val_mae: 2.8662\n",
            "Epoch 94/100\n",
            "162/162 - 0s - loss: 6.0711 - mae: 1.8618 - val_loss: 22.9735 - val_mae: 2.9314\n",
            "Epoch 95/100\n",
            "162/162 - 0s - loss: 5.9069 - mae: 1.8651 - val_loss: 23.7064 - val_mae: 3.0088\n",
            "Epoch 96/100\n",
            "162/162 - 0s - loss: 5.5593 - mae: 1.7522 - val_loss: 24.0032 - val_mae: 2.9360\n",
            "Epoch 97/100\n",
            "162/162 - 0s - loss: 5.3807 - mae: 1.7615 - val_loss: 21.9582 - val_mae: 2.8939\n",
            "Epoch 98/100\n",
            "162/162 - 0s - loss: 5.3251 - mae: 1.7866 - val_loss: 24.3187 - val_mae: 3.0982\n",
            "Epoch 99/100\n",
            "162/162 - 0s - loss: 5.5015 - mae: 1.7849 - val_loss: 24.5360 - val_mae: 3.2092\n",
            "Epoch 100/100\n",
            "162/162 - 0s - loss: 5.3501 - mae: 1.7442 - val_loss: 22.2750 - val_mae: 2.8922\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 14.4099 - mae: 2.5567\n",
            "loss, mae:  14.40992259979248 2.5567142963409424\n",
            "RMSE:  3.79604081933286\n",
            "R2:  0.8004349233068114\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}